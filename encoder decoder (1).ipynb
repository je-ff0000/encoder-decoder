{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde03073-ac20-42b6-ab85-d6c1a60de222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41ed0c4-bbe4-434c-9ed5-34c563978597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into pandas DataFrames\n",
    "input_data = pd.read_csv('input_sentences.csv')\n",
    "target_data = pd.read_csv('target_sentences.csv')\n",
    "\n",
    "# Assuming your CSV files have columns named 'input_sentence' and 'target_sentence'\n",
    "input_sentences = input_data['input_sentence'].tolist()\n",
    "target_sentences = target_data['target_sentence'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ced9987-b840-48a2-9bd5-d5b3ce43c753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asl_gloss_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43masl_gloss_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mword_index\u001b[38;5;241m.\u001b[39mkeys()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'asl_gloss_tokenizer' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b77ea0b-1155-4678-a6ee-3cbd9aeb9994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample English sentences and their ASL gloss translations\n",
    "# english_sentences = [\n",
    "#     \"hello how are you\",\n",
    "#     \"what is your name\",\n",
    "#     \"my name is John\"\n",
    "# ]\n",
    "\n",
    "# asl_gloss_sentences = [\n",
    "#     \"HELLO HOW YOU\",\n",
    "#     \"WHAT YOUR NAME\",\n",
    "#     \"MY NAME JOHN\"\n",
    "# ]\n",
    "\n",
    "# Tokenize English sentences\n",
    "english_tokenizer = Tokenizer(filters='')\n",
    "english_tokenizer.fit_on_texts(input_sentences)\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize ASL gloss sentences\n",
    "asl_gloss_tokenizer = Tokenizer(filters='')\n",
    "asl_gloss_tokenizer.fit_on_texts(target_sentences)\n",
    "asl_gloss_vocab_size = len(asl_gloss_tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text sequences to integer sequences\n",
    "english_sequences = english_tokenizer.texts_to_sequences(input_sentences)\n",
    "asl_gloss_sequences = asl_gloss_tokenizer.texts_to_sequences(target_sentences)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_length = max(len(seq) for seq in english_sequences)\n",
    "english_sequences_padded = pad_sequences(english_sequences, maxlen=max_length, padding='post')\n",
    "asl_gloss_sequences_padded = pad_sequences(asl_gloss_sequences, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe9df5e-c602-4374-bc29-6fcd38ab1015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'hello', 'how', 'you', 'what', 'your', 'my', 'john'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asl_gloss_tokenizer.word_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "975956a9-a38e-4818-b7a6-bc5e2a3643b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(english_sequences_padded, asl_gloss_sequences_padded, test_size=0.2)\n",
    "\n",
    "# Define the encoder-decoder model\n",
    "embedding_dim = 16\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(max_length,))\n",
    "encoder_embedding = tf.keras.layers.Embedding(english_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = tf.keras.layers.LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(max_length,))\n",
    "decoder_embedding = tf.keras.layers.Embedding(asl_gloss_vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(asl_gloss_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a08342-2996-41e8-b5e7-9a9662ab758a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "525a76be-0194-4919-a454-86e2273380c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 193ms/step - accuracy: 0.4051 - loss: 5.7931 - val_accuracy: 0.7701 - val_loss: 5.5913\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7635 - loss: 5.2183 - val_accuracy: 0.7701 - val_loss: 2.2953\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7676 - loss: 1.8635 - val_accuracy: 0.7701 - val_loss: 1.6592\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7764 - loss: 1.5265 - val_accuracy: 0.7701 - val_loss: 1.5055\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7770 - loss: 1.4021 - val_accuracy: 0.7701 - val_loss: 1.5046\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7660 - loss: 1.3994 - val_accuracy: 0.7701 - val_loss: 1.5268\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7686 - loss: 1.4090 - val_accuracy: 0.7739 - val_loss: 1.5371\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7840 - loss: 1.3059 - val_accuracy: 0.7759 - val_loss: 1.5500\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7827 - loss: 1.3076 - val_accuracy: 0.7759 - val_loss: 1.5642\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7807 - loss: 1.2893 - val_accuracy: 0.7759 - val_loss: 1.5782\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7836 - loss: 1.2873 - val_accuracy: 0.7759 - val_loss: 1.5864\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7796 - loss: 1.2713 - val_accuracy: 0.7759 - val_loss: 1.5951\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7736 - loss: 1.2949 - val_accuracy: 0.7759 - val_loss: 1.6058\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7841 - loss: 1.2507 - val_accuracy: 0.7759 - val_loss: 1.6221\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7863 - loss: 1.2297 - val_accuracy: 0.7759 - val_loss: 1.6280\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7821 - loss: 1.2340 - val_accuracy: 0.7759 - val_loss: 1.6359\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7813 - loss: 1.2432 - val_accuracy: 0.7759 - val_loss: 1.6460\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7752 - loss: 1.2633 - val_accuracy: 0.7759 - val_loss: 1.6512\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7696 - loss: 1.2912 - val_accuracy: 0.7759 - val_loss: 1.6587\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7797 - loss: 1.2032 - val_accuracy: 0.7759 - val_loss: 1.6683\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7794 - loss: 1.2267 - val_accuracy: 0.7759 - val_loss: 1.6728\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7737 - loss: 1.2437 - val_accuracy: 0.7759 - val_loss: 1.6812\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7892 - loss: 1.1651 - val_accuracy: 0.7759 - val_loss: 1.6919\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7809 - loss: 1.1997 - val_accuracy: 0.7759 - val_loss: 1.6951\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7780 - loss: 1.2110 - val_accuracy: 0.7759 - val_loss: 1.7009\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7762 - loss: 1.2148 - val_accuracy: 0.7778 - val_loss: 1.7114\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7871 - loss: 1.1557 - val_accuracy: 0.7759 - val_loss: 1.7132\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7729 - loss: 1.2107 - val_accuracy: 0.7759 - val_loss: 1.7160\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7790 - loss: 1.1863 - val_accuracy: 0.7778 - val_loss: 1.7246\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7808 - loss: 1.1723 - val_accuracy: 0.7759 - val_loss: 1.7204\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7859 - loss: 1.1470 - val_accuracy: 0.7778 - val_loss: 1.7245\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7767 - loss: 1.1736 - val_accuracy: 0.7778 - val_loss: 1.7320\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7822 - loss: 1.1441 - val_accuracy: 0.7778 - val_loss: 1.7348\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7796 - loss: 1.1406 - val_accuracy: 0.7778 - val_loss: 1.7381\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7813 - loss: 1.1437 - val_accuracy: 0.7778 - val_loss: 1.7430\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7809 - loss: 1.1368 - val_accuracy: 0.7778 - val_loss: 1.7425\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7759 - loss: 1.1615 - val_accuracy: 0.7778 - val_loss: 1.7439\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7778 - loss: 1.1391 - val_accuracy: 0.7759 - val_loss: 1.7487\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7781 - loss: 1.1395 - val_accuracy: 0.7778 - val_loss: 1.7504\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7833 - loss: 1.1071 - val_accuracy: 0.7778 - val_loss: 1.7515\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7784 - loss: 1.1188 - val_accuracy: 0.7778 - val_loss: 1.7545\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7782 - loss: 1.1231 - val_accuracy: 0.7778 - val_loss: 1.7561\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7754 - loss: 1.1316 - val_accuracy: 0.7778 - val_loss: 1.7606\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7849 - loss: 1.0868 - val_accuracy: 0.7778 - val_loss: 1.7638\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7811 - loss: 1.0979 - val_accuracy: 0.7778 - val_loss: 1.7682\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7851 - loss: 1.0922 - val_accuracy: 0.7778 - val_loss: 1.7714\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7761 - loss: 1.1067 - val_accuracy: 0.7778 - val_loss: 1.7742\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7780 - loss: 1.1044 - val_accuracy: 0.7778 - val_loss: 1.7785\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7771 - loss: 1.1111 - val_accuracy: 0.7778 - val_loss: 1.7796\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7832 - loss: 1.0676 - val_accuracy: 0.7797 - val_loss: 1.7840\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7890 - loss: 1.0514 - val_accuracy: 0.7778 - val_loss: 1.7871\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7890 - loss: 1.0555 - val_accuracy: 0.7778 - val_loss: 1.7870\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7797 - loss: 1.0883 - val_accuracy: 0.7797 - val_loss: 1.7881\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7811 - loss: 1.0798 - val_accuracy: 0.7797 - val_loss: 1.7891\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7843 - loss: 1.0649 - val_accuracy: 0.7778 - val_loss: 1.7939\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7803 - loss: 1.0647 - val_accuracy: 0.7797 - val_loss: 1.7960\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7864 - loss: 1.0446 - val_accuracy: 0.7778 - val_loss: 1.7977\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7720 - loss: 1.1390 - val_accuracy: 0.7759 - val_loss: 1.7984\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7637 - loss: 1.1651 - val_accuracy: 0.7778 - val_loss: 1.8106\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7746 - loss: 1.0962 - val_accuracy: 0.7778 - val_loss: 1.7992\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7807 - loss: 1.0697 - val_accuracy: 0.7778 - val_loss: 1.8079\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7860 - loss: 1.0408 - val_accuracy: 0.7778 - val_loss: 1.7939\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7817 - loss: 1.0477 - val_accuracy: 0.7759 - val_loss: 1.7989\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7894 - loss: 1.0127 - val_accuracy: 0.7797 - val_loss: 1.7919\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7875 - loss: 1.0326 - val_accuracy: 0.7778 - val_loss: 1.8024\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7821 - loss: 1.0381 - val_accuracy: 0.7778 - val_loss: 1.8028\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7936 - loss: 0.9866 - val_accuracy: 0.7739 - val_loss: 1.8072\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7840 - loss: 1.0198 - val_accuracy: 0.7759 - val_loss: 1.8090\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7792 - loss: 1.0352 - val_accuracy: 0.7739 - val_loss: 1.8153\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7880 - loss: 0.9961 - val_accuracy: 0.7778 - val_loss: 1.8075\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7860 - loss: 1.0072 - val_accuracy: 0.7759 - val_loss: 1.8194\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7904 - loss: 0.9678 - val_accuracy: 0.7778 - val_loss: 1.8135\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7860 - loss: 1.0009 - val_accuracy: 0.7778 - val_loss: 1.8128\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7858 - loss: 0.9998 - val_accuracy: 0.7778 - val_loss: 1.8227\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7880 - loss: 0.9778 - val_accuracy: 0.7797 - val_loss: 1.8136\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7986 - loss: 0.9519 - val_accuracy: 0.7778 - val_loss: 1.8252\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7834 - loss: 0.9901 - val_accuracy: 0.7778 - val_loss: 1.8328\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7871 - loss: 0.9738 - val_accuracy: 0.7797 - val_loss: 1.8219\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7804 - loss: 1.0079 - val_accuracy: 0.7778 - val_loss: 1.8518\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7861 - loss: 0.9943 - val_accuracy: 0.7778 - val_loss: 1.8274\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7919 - loss: 0.9613 - val_accuracy: 0.7739 - val_loss: 1.8554\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7803 - loss: 0.9962 - val_accuracy: 0.7759 - val_loss: 1.8299\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7820 - loss: 1.0115 - val_accuracy: 0.7778 - val_loss: 1.8528\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7970 - loss: 0.9272 - val_accuracy: 0.7778 - val_loss: 1.8342\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7879 - loss: 0.9602 - val_accuracy: 0.7759 - val_loss: 1.8624\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7917 - loss: 0.9484 - val_accuracy: 0.7816 - val_loss: 1.8333\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7998 - loss: 0.9294 - val_accuracy: 0.7759 - val_loss: 1.8575\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7951 - loss: 0.9323 - val_accuracy: 0.7797 - val_loss: 1.8373\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7969 - loss: 0.9310 - val_accuracy: 0.7797 - val_loss: 1.8590\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7926 - loss: 0.9498 - val_accuracy: 0.7797 - val_loss: 1.8437\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8033 - loss: 0.9183 - val_accuracy: 0.7797 - val_loss: 1.8594\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8017 - loss: 0.9080 - val_accuracy: 0.7816 - val_loss: 1.8496\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8003 - loss: 0.9173 - val_accuracy: 0.7797 - val_loss: 1.8662\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7995 - loss: 0.9036 - val_accuracy: 0.7816 - val_loss: 1.8536\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8010 - loss: 0.9128 - val_accuracy: 0.7797 - val_loss: 1.8680\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8063 - loss: 0.8937 - val_accuracy: 0.7816 - val_loss: 1.8577\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8004 - loss: 0.9187 - val_accuracy: 0.7797 - val_loss: 1.8795\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8048 - loss: 0.8932 - val_accuracy: 0.7797 - val_loss: 1.8599\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8013 - loss: 0.9021 - val_accuracy: 0.7797 - val_loss: 1.8844\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8066 - loss: 0.8726 - val_accuracy: 0.7797 - val_loss: 1.8644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cd4eff7910>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit([X_train, X_train], np.expand_dims(y_train, axis=-1), epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0f0ebaf-4a4f-4a35-9ffa-3ec645872df9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7694 - loss: 2.2519\n",
      "Test Accuracy: 0.7669753432273865\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test, X_test], np.expand_dims(y_test, axis=-1))\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f43fc5b3-f1bb-4539-8892-02e1ea3d21ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "path = 'encoddecod/eng2gloss.h5'\n",
    "model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9dcf7a1-cc84-4dc0-ac2f-ebb80547d288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<start>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m input_sequence_padded \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequence, maxlen\u001b[38;5;241m=\u001b[39mmax_length_src, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize decoder input sequence with start token\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[43masl_gloss_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<start>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m]])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Encode the input sequence\u001b[39;00m\n\u001b[0;32m     14\u001b[0m encoder_input \u001b[38;5;241m=\u001b[39m input_sequence_padded\n",
      "\u001b[1;31mKeyError\u001b[0m: '<start>'"
     ]
    }
   ],
   "source": [
    "max_length_src = max(len(seq) for seq in english_sequences_padded)\n",
    "\n",
    "input_sentence = \"what is your name\"\n",
    "\n",
    "# Tokenize input sentence\n",
    "input_sequence = english_tokenizer.texts_to_sequences([input_sentence])\n",
    "input_sequence_padded = pad_sequences(input_sequence, maxlen=max_length_src, padding='post')\n",
    "\n",
    "# Initialize decoder input sequence with start token\n",
    "decoder_input = np.array([[asl_gloss_tokenizer.word_index['<start>']]])\n",
    "\n",
    "\n",
    "# Encode the input sequence\n",
    "encoder_input = input_sequence_padded\n",
    "reshaped_encoder_input = tf.reshape(encoder_input, (batch_size, 1, encoder_input.shape[-1]))\n",
    "encoder_output, state_h_enc, state_c_enc = encoder_lstm(reshaped_encoder_input)\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "\n",
    "\n",
    "\n",
    "# Generate ASL gloss sequence\n",
    "decoded_sentence = ''\n",
    "while True:\n",
    "    decoder_output, state_h_dec, state_c_dec = decoder_lstm(tf.expand_dims(decoder_input, axis=-1), initial_state=encoder_states)\n",
    "    decoder_states = [state_h_dec, state_c_dec]\n",
    "    decoder_prediction = decoder_dense(decoder_output)\n",
    "    predicted_word_index = tf.argmax(decoder_prediction, axis=-1).numpy()[0][0]\n",
    "    predicted_word = asl_gloss_tokenizer.index_word[predicted_word_index]\n",
    "    if predicted_word == '<end>' or len(decoded_sentence.split()) > max_length_tar:\n",
    "        break\n",
    "    decoded_sentence += predicted_word + ' '\n",
    "    # Update decoder input for the next iteration\n",
    "    decoder_input = np.array([[predicted_word_index]])\n",
    "\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Predicted ASL Gloss:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca7353b-e720-4a82-97fd-44cb59178d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<hello>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m input_sequence_padded \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequence, maxlen\u001b[38;5;241m=\u001b[39mmax_length_src, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize decoder input sequence with start token\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[43masl_gloss_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m]\u001b[49m]])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Encode the input sequence\u001b[39;00m\n\u001b[0;32m     20\u001b[0m encoder_input \u001b[38;5;241m=\u001b[39m input_sequence_padded\n",
      "\u001b[1;31mKeyError\u001b[0m: '<hello>'"
     ]
    }
   ],
   "source": [
    "max_length_src = max(len(seq) for seq in english_sequences_padded)\n",
    "max_num_words=4\n",
    "\n",
    "input_sentence = \"what is your name\"\n",
    "\n",
    "start_token = '<start>'\n",
    "asl_gloss_tokenizer = Tokenizer(num_words=max_num_words, oov_token='<unk>')  # Adjust parameters as needed\n",
    "asl_gloss_tokenizer.fit_on_texts([start_token] + asl_gloss_sentences)\n",
    "\n",
    "\n",
    "# Tokenize input sentence\n",
    "input_sequence = english_tokenizer.texts_to_sequences([input_sentence])\n",
    "input_sequence_padded = pad_sequences(input_sequence, maxlen=max_length_src, padding='post')\n",
    "\n",
    "# Initialize decoder input sequence with start token\n",
    "decoder_input = np.array([[asl_gloss_tokenizer.word_index[start_token]]])\n",
    "\n",
    "\n",
    "# Encode the input sequence\n",
    "encoder_input = input_sequence_padded\n",
    "reshaped_encoder_input = tf.reshape(encoder_input, (batch_size, 1, encoder_input.shape[-1]))\n",
    "encoder_output, state_h_enc, state_c_enc = encoder_lstm(reshaped_encoder_input)\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "\n",
    "\n",
    "\n",
    "# Generate ASL gloss sequence\n",
    "decoded_sentence = ''\n",
    "while True:\n",
    "    decoder_output, state_h_dec, state_c_dec = decoder_lstm(tf.expand_dims(decoder_input, axis=-1), initial_state=encoder_states)\n",
    "    decoder_states = [state_h_dec, state_c_dec]\n",
    "    decoder_prediction = decoder_dense(decoder_output)\n",
    "    predicted_word_index = tf.argmax(decoder_prediction, axis=-1).numpy()[0][0]\n",
    "    predicted_word = asl_gloss_tokenizer.index_word[predicted_word_index]\n",
    "    if predicted_word == '<end>' or len(decoded_sentence.split()) > max_length_tar:\n",
    "        break\n",
    "    decoded_sentence += predicted_word + ' '\n",
    "    # Update decoder input for the next iteration\n",
    "    decoder_input = np.array([[predicted_word_index]])\n",
    "\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Predicted ASL Gloss:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f17b226-7204-44a9-bddc-4d9471c19b51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer 'functional_1' expected 2 input(s). Received 1 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Test the translation function with an example English sentence\u001b[39;00m\n\u001b[0;32m     29\u001b[0m english_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello how are you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m predicted_asl_gloss \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_to_asl\u001b[49m\u001b[43m(\u001b[49m\u001b[43menglish_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish Sentence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, english_sentence)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted ASL Gloss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_asl_gloss)\n",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m, in \u001b[0;36mtranslate_to_asl\u001b[1;34m(english_sentence)\u001b[0m\n\u001b[0;32m     18\u001b[0m input_sequence_padded \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequence, maxlen\u001b[38;5;241m=\u001b[39mmax_length_src, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m predicted_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence_padded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Convert predicted token indices to ASL gloss text\u001b[39;00m\n\u001b[0;32m     24\u001b[0m asl_gloss_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([asl_gloss_tokenizer\u001b[38;5;241m.\u001b[39mindex_word[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m predicted_sequence[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[1;32mD:\\encoder\\encoddecod\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\encoder\\encoddecod\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:156\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_spec) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs):\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# Having a shape/dtype is the only commonality of the various\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# tensor-like objects that may be passed. The most common kind of\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# invalid type we are guarding for is a Layer instance (Functional API),\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# which does not have a `shape` attribute.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Layer 'functional_1' expected 2 input(s). Received 1 instead."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the saved model\n",
    "# model = load_model('path/to/saved/model.h5')\n",
    "\n",
    "# Define English and ASL gloss tokenizers\n",
    "# Replace these with your actual tokenizer instances\n",
    "\n",
    "\n",
    "# Define a function to translate English sentence to ASL gloss text\n",
    "def translate_to_asl(english_sentence):\n",
    "    # Tokenize input sentence\n",
    "    input_sequence = english_tokenizer.texts_to_sequences([english_sentence])\n",
    "    \n",
    "    # Pad sequences to ensure uniform length\n",
    "    max_length_src = max(len(seq) for seq in english_sequences_padded)  # Define the maximum sequence length used during training\n",
    "    input_sequence_padded = pad_sequences(input_sequence, maxlen=max_length_src, padding='post')\n",
    "    \n",
    "    # Generate predictions\n",
    "    predicted_sequence = model.predict(input_sequence_padded,\"\")\n",
    "    \n",
    "    # Convert predicted token indices to ASL gloss text\n",
    "    asl_gloss_text = ' '.join([asl_gloss_tokenizer.index_word[idx] for idx in predicted_sequence[0]])\n",
    "    \n",
    "    return asl_gloss_text\n",
    "\n",
    "# Test the translation function with an example English sentence\n",
    "english_sentence = \"hello how are you\"\n",
    "predicted_asl_gloss = translate_to_asl(english_sentence)\n",
    "print(\"English Sentence:\", english_sentence)\n",
    "print(\"Predicted ASL Gloss:\", predicted_asl_gloss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encoddecod",
   "language": "python",
   "name": "encoddecod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
